{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gauri131202/JPMC_Final_Project/blob/main/case_study_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4NZEAtVi7XZ",
        "outputId": "cf0cd7cc-4510-481a-d7b5-9d152d598b40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#connecting to google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing all the necessary libraries\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "id": "BGZCUkPdjJ5D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cleaning and preprocessing the images"
      ],
      "metadata": {
        "id": "uL7qJNdWl_HY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#cleaning the image\n",
        "# Function to apply thresholding and opening to an image\n",
        "def preprocess_image(image):\n",
        "\n",
        "    # Apply OTSU thresholding\n",
        "    _, thresholded = cv2.threshold(image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "\n",
        "\n",
        "    # Define a kernel for the opening operation\n",
        "    kernel = np.ones((2, 2), np.uint8)\n",
        "\n",
        "\n",
        "    opened = cv2.morphologyEx(thresholded, cv2.MORPH_OPEN, kernel)\n",
        "\n",
        "    return opened\n",
        "\n",
        "#Path to the directory containing the image dataset\n",
        "dataset_dir = \"/content/drive/MyDrive/JPMC/samples\"\n",
        "\n",
        "preprocessed_array= []\n",
        "\n",
        "# Loop through the images in the dataset directory\n",
        "for filename in os.listdir(dataset_dir):\n",
        "    if filename.endswith(\".png\"):\n",
        "        # Read the image\n",
        "        image_path = os.path.join(dataset_dir, filename)\n",
        "        image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "        # Preprocess the image\n",
        "        preprocessed_image = preprocess_image(image)\n",
        "\n",
        "        # Save the preprocessed image\n",
        "        preprocessed_array.append(preprocessed_image)\n",
        "\n",
        "print(len(preprocessed_array))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KrsxSqxDjVYs",
        "outputId": "432d9ff9-36df-4455-f178-3f7a16ab5f0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#extracting the file names to train the models\n",
        "\n",
        "filename_characters = []\n",
        "filenames = []\n",
        "\n",
        "for file in os.listdir(dataset_dir):\n",
        "  filename, extension = os.path.splitext(file)\n",
        "  filenames.append(filename)\n",
        "\n",
        "for names in filenames:\n",
        "  string = names\n",
        "\n",
        "  # Convert the string to a list of characters\n",
        "  characters = list(string)\n",
        "\n",
        "  for character in characters:\n",
        "    filename_characters.append(character)\n",
        "\n",
        "len(filename_characters)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-kRdYQNm816",
        "outputId": "afbc0528-adb0-46cb-a099-1ff7d945b07e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5000"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dividing each image into 5 parts of 100X20 pixels"
      ],
      "metadata": {
        "id": "1eA5lXPrmE8C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#cropping the image into 5 parts\n",
        "\n",
        "cropped_array = []\n",
        "for i in range(len(preprocessed_array)):\n",
        "  img = preprocessed_array[i]\n",
        "\n",
        "  width = 20\n",
        "\n",
        "    # Iterate over the width of the image to extract character images\n",
        "  for x in range(5):\n",
        "        # Calculate the starting and ending coordinates for each character\n",
        "      start = x * width\n",
        "      end = start + width\n",
        "\n",
        "        # Extract the character image\n",
        "      cropped = img[0:100, start:end]\n",
        "      #character_image = img.crop((start_x, 0, end_x, height))\n",
        "\n",
        "      cropped_array.append(cropped)\n",
        "\n",
        "print(len(cropped_array))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pKRmypt8jfAh",
        "outputId": "80815bc0-52cb-4a9a-f9bb-21d21c1bc365"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model 1: Logistic Regression"
      ],
      "metadata": {
        "id": "LGeQH6MOlK0_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing the required libraries\n",
        "import scipy\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression"
      ],
      "metadata": {
        "id": "J-_YghYblQro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading the dataset\n",
        "X = np.array(cropped_array)\n",
        "y = np.array(filename_characters)\n",
        "\n",
        "#Splitting the dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=45)\n",
        "\n",
        "#Reshaping the images\n",
        "X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
        "X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
        "\n",
        "#Training the model\n",
        "classifier = LogisticRegression()\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "#Running on the test set\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "#Evaluating the Logistic Regression model\n",
        "accuracy = classifier.score(X_test, y_test)\n",
        "print(\"Accuracy: \", accuracy)\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_score(y_test,y_pred)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O79WlsRblYei",
        "outputId": "f41865f7-2fa2-4519-a975-224b231fa4e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:  0.477\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.477"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model 2: Support Vector Machines"
      ],
      "metadata": {
        "id": "buz0qvS8tPfe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing the required libraries\n",
        "from sklearn import svm\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "XIO2t6RWtUsp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading the dataset\n",
        "X = np.array(cropped_array)\n",
        "Y = np.array(filename_characters)\n",
        "\n",
        "#Splitting the data into training and testing sets\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "#Reshaping the images\n",
        "X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
        "X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
        "\n",
        "#Creating an SVM classifier\n",
        "classifier = svm.SVC()\n",
        "\n",
        "#Training the SVM classifier\n",
        "classifier.fit(X_train, Y_train)\n",
        "\n",
        "#Predicting the labels for the test set\n",
        "Y_pred = classifier.predict(X_test)\n",
        "\n",
        "#Evaluating the accuracy of the classifier\n",
        "accuracy = classifier.score(X_test, Y_test)\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7_CwSDCtYsl",
        "outputId": "a119c363-39b0-43df-a251-163227e4722a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.653\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model 3: Random Forest"
      ],
      "metadata": {
        "id": "xmkK3VJgtiqj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing the required libraries\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score"
      ],
      "metadata": {
        "id": "BsRQuValtpBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading the dataset\n",
        "X = np.array(cropped_array)\n",
        "Y = np.array(filename_characters)\n",
        "\n",
        "#Splitting the data into training and testing sets\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "#Reshaping the images\n",
        "X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
        "X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
        "\n",
        "#Creating a Random Forest classifier\n",
        "classifier = RandomForestClassifier()\n",
        "\n",
        "#Training the Random Forest classifier\n",
        "classifier.fit(X_train, Y_train)\n",
        "\n",
        "#Predicting the labels for the test set\n",
        "Y_pred = classifier.predict(X_test)\n",
        "\n",
        "#Evaluating the accuracy of the classifier\n",
        "accuracy = classifier.score(X_test, Y_test)\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0cfCwKOGtpMc",
        "outputId": "0e39906d-2d92-4e47-9326-2f04bc1054f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.672\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Hyperparameter tuning\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Loading the dataset\n",
        "X = np.array(cropped_array)\n",
        "Y = np.array(filename_characters)\n",
        "\n",
        "# Splitting the data into training and testing sets\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Reshaping the images\n",
        "X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
        "X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Creating a Random Forest classifier\n",
        "classifier = RandomForestClassifier(n_estimators=600,max_depth=30,min_samples_split=5 ,min_samples_leaf=1,max_features='sqrt')\n",
        "\n",
        "# Training the Random Forest classifier\n",
        "classifier.fit(X_train, Y_train)\n",
        "\n",
        "# Predicting the labels for the test set\n",
        "Y_pred = classifier.predict(X_test)\n",
        "\n",
        "# Evaluating the accuracy of the classifier\n",
        "accuracy = accuracy_score(Y_test, Y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_gCAPK0RSwr",
        "outputId": "9fb85490-02e2-415b-cbc5-a895146035c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.718\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Answer to the questions mentioned:**"
      ],
      "metadata": {
        "id": "9bo8XHOJvLUh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Which of the above classifiers yield the best accuracy on the validation set? Do you think accuracy is the best metric to compare the algorithms, if not - which other metric could be used?\n",
        "\n",
        "  The Random Forest classifier yielded the best accuracy- 0.678 -on the validation set.\n",
        "  \n",
        "  No, accuracy is not always the best metric to compare models.\n",
        "  Here are a few commonly used metrics:\n",
        "  1. **Precision**: Precision measures the proportion of correctly predicted positive instances (true positives) out of the total instances predicted as positive (true positives + false positives). It focuses on the model's ability to avoid false positives.\n",
        "  2. **Recall** (Sensitivity or True Positive Rate): Recall calculates the proportion of correctly predicted positive instances (true positives) out of the total actual positive instances (true positives + false negatives). It emphasizes the model's ability to identify all positive instances.\n",
        "  3. **F1 Score**: The F1 score is the harmonic mean of precision and recall. It provides a balanced measure that considers both precision and recall. It is useful when there is an uneven distribution of classes or when you want to find a balance between precision and recall.\n",
        "  4. **Area Under the ROC Curve** (AUC-ROC): AUC-ROC measures the performance of a binary classifier at various classification thresholds. It considers the true positive rate (sensitivity) against the false positive rate (1 - specificity) across different thresholds. It provides an aggregate measure of the model's performance across all possible thresholds.\n",
        "  5. **Confusion Matrix**: A confusion matrix is a tabular representation that shows the counts of true positives, true negatives, false positives, and false negatives. It provides a detailed breakdown of the model's predictions and can be used to calculate metrics like accuracy, precision, recall, and F1 score."
      ],
      "metadata": {
        "id": "LDgCP8JJvVTP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Tweak the hyperparameters of the classifier selected in Q1 and try to improve the accuracy. What hyperparameters did you change, why do you think it worked?\n",
        "\n",
        "  <Answer> To tweak the hyperparameters of the Random Forest classifier and potentially improve the accuracy, the following parameters can be modified:\n",
        "\n",
        "  1.   **n_estimators**: The number of decision trees being built in the forest. Default values in sklearn are 100. N_estimators are mostly correlated to the size of data, to encapsulate the trends in the data, more number of DTs are needed.\n",
        "\n",
        "  2.   **max_depth**: It controls the maximum depth of each decision tree in the forest. Increasing max_depth allows the trees to capture more complex relationships in the data but also increases the risk of overfitting.\n",
        "\n",
        "  3. **min_samples_split**: This parameter determines the minimum number of samples required to split an internal node. Increasing this value can help prevent overfitting by requiring more samples for a split.\n",
        "\n",
        "  4. **min_samples_leaf**: It sets the minimum number of samples required to be at a leaf node. Similar to min_samples_split, increasing min_samples_leaf can prevent overfitting by enforcing a minimum number of samples in each leaf.\n",
        "\n",
        "  5. **max_features**: This parameter controls the number of features to consider when looking for the best split. Reducing the number of features can help prevent overfitting, especially if the dataset has many features.\n",
        "\n",
        "After playing around with the parameter values, and using trial and error methods, we came up with the most optimised values as follows:\n",
        "\n",
        "`classifier = RandomForestClassifier(n_estimators=600,max_depth=30,min_samples_split=5 ,min_samples_leaf=1,max_features='sqrt')`\n",
        "\n",
        "This improved the accuracy of the model to 0.718\n"
      ],
      "metadata": {
        "id": "zIQbUmzNzwPM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Find Precision, Recall and F1 score in all cases.\n",
        "\n",
        "  <Answer>\n",
        "  1. **Random Forest:**\n",
        "    * Precision: 0.7449812348667282\n",
        "    * Recall: 0.7153426940633367\n",
        "    * F1 Score: 0.7203288485030938\n",
        "\n",
        "  2. **Support Vector Machine:**\n",
        "    * Precision: 0.6796350234338434\n",
        "    * Recall: 0.6492042178366149\n",
        "    * F1 Score: 0.6547169511226476\n",
        "  \n",
        "  3. **Logistic Regression:**\n",
        "    * Precision: 0.5920632023311434\n",
        "    * Recall: 0.5582204276332229\n",
        "    * F1 Score: 0.5767232124510447\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DCJxJpphz0jM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. a. Find the confusion matrix using the best classifier.\n",
        "  \n",
        "  b. Which characters have low accuracy?\n",
        "  \n",
        "  c. What can be the reason for this?\n",
        "\n",
        "  <Answer>"
      ],
      "metadata": {
        "id": "bnDi37tLz_lU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "  a. The confusion matrix can be found using the code\n",
        "`confusion_mat = confusion_matrix(Y_test, Y_pred)`, and importing the library   confusion_matrix from sklearn.metrics\n",
        "\n",
        "The code is as follows:\n"
      ],
      "metadata": {
        "id": "4GhUnXhRKkNU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing the required libraries\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "X = np.array(cropped_array)\n",
        "Y = np.array(filename_characters)\n",
        "\n",
        "#Splitting the data into training and testing sets\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "#Reshaping the images\n",
        "X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
        "X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
        "\n",
        "#Creating a Random Forest classifier\n",
        "classifier = RandomForestClassifier()\n",
        "\n",
        "#Training the Random Forest classifier\n",
        "classifier.fit(X_train, Y_train)\n",
        "\n",
        "#Predicting the labels for the test set\n",
        "Y_pred = classifier.predict(X_test)\n",
        "confusion_mat = confusion_matrix(Y_test, Y_pred)\n",
        "\n",
        "# Printing the confusion matrix\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_mat)"
      ],
      "metadata": {
        "id": "04aJFF2qKoZf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4c13a36-b88a-4caa-8f39-0ed53a1c4b6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "[[38  0  0  0  0  0  0  0  2  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   0  2]\n",
            " [ 1 31  0  1  0  1  0  2  2  0  1  2  0  0  0  0  0  0  0  1  0  0  0  0\n",
            "   0  1]\n",
            " [ 0  0 31  0  0  1  0  0  1  1  0  2  0  0  1  0  0  1  0  0  0  0  0  0\n",
            "   0  0]\n",
            " [ 0  2  0 25  0  3  0  0  1  1  0  0  1  0  0  0  0  0  0  1  0  0  1  0\n",
            "   0  1]\n",
            " [ 5  0  0  0 28  0  0  0  0  1  0  0  0  0  0  0  0  0  1  0  0  0  0  0\n",
            "   0  1]\n",
            " [ 0  0  0  0  0 24  0  0  2  4  1  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "   2  1]\n",
            " [ 1  1  0  0  1  0 21  0  0  2  0  0  0  0  0  0  0  0  1  1  0  0  0  1\n",
            "   0  1]\n",
            " [ 0  9  0  3  0  4  0 20  1  0  0  0  0  0  0  0  0  0  0  0  0  1  0  1\n",
            "   0  0]\n",
            " [ 0  0  0  0  0  4  0  0 16  1  0  3  0  0  0  0  0  0  0  0  0  0  0  3\n",
            "   0  0]\n",
            " [ 0  1  0  0  0  1  0  0  0 42  0  1  0  0  0  1  0  0  0  0  0  1  0  0\n",
            "   0  0]\n",
            " [ 0  0  1  1  0  0  0  0  2  0 31  1  0  0  0  0  0  0  0  1  0  3  0  6\n",
            "   1  0]\n",
            " [ 0  0  0  0  0  3  0  0  5  5  0 24  0  0  0  0  0  1  0  0  0  0  0  1\n",
            "   0  1]\n",
            " [ 1  0  2  0  0  0  0  0  0  2  0  0 23  0  0  0  0  0  0  0  0  0  7  0\n",
            "   1  1]\n",
            " [ 1  0  3  2  1  1  0  1  0  0  1  1  0 27  5  0  0  0  0  1  0  1  0  0\n",
            "   1  0]\n",
            " [ 0  1  4  0  0  0  0  0  1  0  0  0  0  3 25  0  0  0  1  0  0  0  0  0\n",
            "   0  0]\n",
            " [ 0  0  1  0  0  0  2  0  0  0  1  1  0  0  0 24  4  0  0  0  0  0  0  0\n",
            "   0  2]\n",
            " [ 0  0  0  0  1  1  0  0  0  0  0  1  0  0  0  4 23  0  0  0  0  0  0  0\n",
            "   0  0]\n",
            " [ 0  0  3  0  0  4  0  0  0  0  0  1  0  0  0  0  0 19  0  0  0  0  0  0\n",
            "   1  0]\n",
            " [ 0  0  1  1  0  1  0  0  3  2  1  3  2  1  0  0  0  0 24  0  0  0  0  1\n",
            "   1  0]\n",
            " [ 1  0  0  1  0  6  0  0  1  2  0  2  0  0  0  0  0  3  0 23  0  0  0  1\n",
            "   0  0]\n",
            " [ 2  1  3  1  1  0  0  0  1  0  0  1  1  1  3  0  1  0  0  0 27  1  0  0\n",
            "   0  0]\n",
            " [ 0  0  1  0  0  0  0  1  1  4  1  0  1  0  0  1  2  0  0  1  0 26  1  0\n",
            "   1  0]\n",
            " [ 1  0  2  0  0  0  0  0  0  1  0  0  1  0  0  0  0  0  0  0  0  0 27  1\n",
            "   0  0]\n",
            " [ 0  0  0  0  1  2  1  0  1  7  0  1  0  0  1  0  0  1  0  0  0  0  1 21\n",
            "   0  1]\n",
            " [ 0  0  1  0  2  2  0  0  1  4  1  3  0  0  0  0  0  3  0  0  0  0  0  2\n",
            "  23  1]\n",
            " [ 1  0  1  0  2  1  0  0  1  1  2  4  1  0  0  0  0  1  0  0  0  0  0  0\n",
            "   0 34]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "b. We found the accuracy for each character by dividing the values on the diagonal with the total number of samples per character. This gave us the accuracy for each character. The code is as follows:"
      ],
      "metadata": {
        "id": "Xwn-ymQpAN0E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing the required libraries\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "X = np.array(cropped_array)\n",
        "Y = np.array(filename_characters)\n",
        "\n",
        "#Splitting the data into training and testing sets\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "#Reshaping the images\n",
        "X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
        "X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
        "\n",
        "#Creating a Random Forest classifier\n",
        "classifier = RandomForestClassifier()\n",
        "\n",
        "#Training the Random Forest classifier\n",
        "classifier.fit(X_train, Y_train)\n",
        "\n",
        "#Predicting the labels for the test set\n",
        "Y_pred = classifier.predict(X_test)\n",
        "confusion_mat = confusion_matrix(Y_test, Y_pred)\n",
        "\n",
        "\n",
        "def calculate_accuracy(confusion_mat, character_index):\n",
        "    successes = confusion_mat[character_index, character_index]\n",
        "    total_samples = np.sum(confusion_mat[character_index, :])\n",
        "    accuracy = successes / total_samples\n",
        "    return accuracy\n",
        "\n",
        "# Calculate accuracy for each character\n",
        "num_characters = confusion_mat.shape[0]\n",
        "accuracies = []\n",
        "for i in range(num_characters):\n",
        "    accuracy = calculate_accuracy(confusion_mat, i)\n",
        "    accuracies.append(accuracy)\n",
        "\n",
        "# Print accuracies\n",
        "for i, accuracy in enumerate(accuracies):\n",
        "    print(f\"Character {i}: {accuracy*100:.2f}%\")"
      ],
      "metadata": {
        "id": "7YN98yx4BQZH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c1ce956-0827-4168-d0d8-504d5b554de3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Character 0: 88.37%\n",
            "Character 1: 65.12%\n",
            "Character 2: 84.21%\n",
            "Character 3: 69.44%\n",
            "Character 4: 72.22%\n",
            "Character 5: 67.65%\n",
            "Character 6: 73.33%\n",
            "Character 7: 43.59%\n",
            "Character 8: 59.26%\n",
            "Character 9: 91.49%\n",
            "Character 10: 59.57%\n",
            "Character 11: 67.50%\n",
            "Character 12: 51.35%\n",
            "Character 13: 54.35%\n",
            "Character 14: 71.43%\n",
            "Character 15: 65.71%\n",
            "Character 16: 73.33%\n",
            "Character 17: 67.86%\n",
            "Character 18: 60.98%\n",
            "Character 19: 57.50%\n",
            "Character 20: 61.36%\n",
            "Character 21: 63.41%\n",
            "Character 22: 84.85%\n",
            "Character 23: 60.53%\n",
            "Character 24: 51.16%\n",
            "Character 25: 63.27%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This clearly shows that character 7,8,11,13,19,and 23, i.e '**h', 'i', 'l', 'n', 't', and 'x'** have low accuracy"
      ],
      "metadata": {
        "id": "Sy0B3FzrCAqJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "c. Few of the reasons for low acccuracy of these alphabets could be:\n",
        "\n",
        "1. Class imbalance: If the dataset has imbalanced class distributions, with some characters having significantly fewer samples compared to others, the classifier may be biased towards the majority classes, resulting in lower accuracy for the minority classes.\n",
        "2. Variability in writing styles or fonts: If the dataset contains characters written in different styles or fonts, it can introduce additional variations and challenges for the classifier to generalize well, resulting in lower accuracy.\n",
        "3. Insufficient training data: If there is limited training data available for certain characters, the classifier may not have enough examples to learn their distinguishing features effectively, leading to lower accuracy.\n",
        "4. Feature representation: The chosen features for character representation may not capture the essential characteristics that differentiate certain characters effectively, leading to lower accuracy. Improving the feature representation or using more advanced feature extraction techniques can potentially address this issue."
      ],
      "metadata": {
        "id": "teetF4e8D81z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Try any other techniques/algorithms in your research that could improve the accuracy.\n",
        "\n",
        "  One way to improve the accuracy is applying Convolutional Nueral Network(CNN) to the dataset.\n"
      ],
      "metadata": {
        "id": "Akos4e86U_m_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Assuming 'cropped_array' is your input data with dimensions 4000x2000\n",
        "X = np.array(cropped_array)\n",
        "Y = np.array(filename_characters)\n",
        "\n",
        "# Use LabelEncoder to encode character labels into numeric values\n",
        "label_encoder = LabelEncoder()\n",
        "Y_encoded = label_encoder.fit_transform(Y)\n",
        "\n",
        "# Splitting the data into training and testing sets\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the dimensions of each sample\n",
        "sample_width = 32\n",
        "sample_height = 32\n",
        "\n",
        "# Resize and reshape the training set\n",
        "X_train_resized = []\n",
        "for img in X_train:\n",
        "    # Resize the image to (32, 32)\n",
        "    resized_img = cv2.resize(img, (32, 32), interpolation=cv2.INTER_AREA)\n",
        "    # Reshape the grayscale image to (32, 32, 1)\n",
        "    reshaped_img = np.reshape(resized_img, (32, 32, 1))\n",
        "    X_train_resized.append(reshaped_img)\n",
        "X_train_resized = np.array(X_train_resized)\n",
        "\n",
        "# Resize and reshape the test set\n",
        "X_test_resized = []\n",
        "for img in X_test:\n",
        "    # Resize the image to (32, 32)\n",
        "    resized_img = cv2.resize(img, (32, 32), interpolation=cv2.INTER_AREA)\n",
        "    # Reshape the grayscale image to (32, 32, 1)\n",
        "    reshaped_img = np.reshape(resized_img, (32, 32, 1))\n",
        "    X_test_resized.append(reshaped_img)\n",
        "X_test_resized = np.array(X_test_resized)\n",
        "\n",
        "# Create a CNN model\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(32, 32, 1)))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(len(label_encoder.classes_), activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train_resized, to_categorical(Y_train), batch_size=32, epochs=50, verbose=1, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = model.evaluate(X_test_resized, to_categorical(Y_test))[1]\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSvg_t6q_bNj",
        "outputId": "78763037-43a6-4f98-ed90-d82f4045ebef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "100/100 [==============================] - 6s 53ms/step - loss: 52.5272 - accuracy: 0.0584 - val_loss: 3.2272 - val_accuracy: 0.0437\n",
            "Epoch 2/50\n",
            "100/100 [==============================] - 3s 31ms/step - loss: 3.1318 - accuracy: 0.0669 - val_loss: 3.1334 - val_accuracy: 0.0538\n",
            "Epoch 3/50\n",
            "100/100 [==============================] - 3s 29ms/step - loss: 2.9618 - accuracy: 0.1241 - val_loss: 3.0455 - val_accuracy: 0.0925\n",
            "Epoch 4/50\n",
            "100/100 [==============================] - 3s 29ms/step - loss: 2.7275 - accuracy: 0.1966 - val_loss: 2.9304 - val_accuracy: 0.1538\n",
            "Epoch 5/50\n",
            "100/100 [==============================] - 5s 47ms/step - loss: 2.4995 - accuracy: 0.2466 - val_loss: 2.7777 - val_accuracy: 0.2013\n",
            "Epoch 6/50\n",
            "100/100 [==============================] - 3s 34ms/step - loss: 2.3079 - accuracy: 0.2916 - val_loss: 2.7260 - val_accuracy: 0.2275\n",
            "Epoch 7/50\n",
            "100/100 [==============================] - 3s 31ms/step - loss: 2.1212 - accuracy: 0.3375 - val_loss: 2.5965 - val_accuracy: 0.2425\n",
            "Epoch 8/50\n",
            "100/100 [==============================] - 3s 31ms/step - loss: 1.9398 - accuracy: 0.4041 - val_loss: 2.5143 - val_accuracy: 0.2663\n",
            "Epoch 9/50\n",
            "100/100 [==============================] - 4s 44ms/step - loss: 1.8003 - accuracy: 0.4425 - val_loss: 2.6648 - val_accuracy: 0.3075\n",
            "Epoch 10/50\n",
            "100/100 [==============================] - 4s 36ms/step - loss: 1.6758 - accuracy: 0.4828 - val_loss: 2.6234 - val_accuracy: 0.3100\n",
            "Epoch 11/50\n",
            "100/100 [==============================] - 3s 31ms/step - loss: 1.5358 - accuracy: 0.5269 - val_loss: 2.8057 - val_accuracy: 0.3400\n",
            "Epoch 12/50\n",
            "100/100 [==============================] - 3s 31ms/step - loss: 1.4117 - accuracy: 0.5622 - val_loss: 2.5997 - val_accuracy: 0.3600\n",
            "Epoch 13/50\n",
            "100/100 [==============================] - 4s 42ms/step - loss: 1.2889 - accuracy: 0.6056 - val_loss: 2.6990 - val_accuracy: 0.3738\n",
            "Epoch 14/50\n",
            "100/100 [==============================] - 4s 39ms/step - loss: 1.2028 - accuracy: 0.6331 - val_loss: 2.9144 - val_accuracy: 0.3600\n",
            "Epoch 15/50\n",
            "100/100 [==============================] - 3s 31ms/step - loss: 1.0904 - accuracy: 0.6734 - val_loss: 3.0413 - val_accuracy: 0.3775\n",
            "Epoch 16/50\n",
            "100/100 [==============================] - 3s 30ms/step - loss: 1.0031 - accuracy: 0.6956 - val_loss: 2.9320 - val_accuracy: 0.4137\n",
            "Epoch 17/50\n",
            "100/100 [==============================] - 4s 37ms/step - loss: 0.9419 - accuracy: 0.7169 - val_loss: 2.9356 - val_accuracy: 0.4175\n",
            "Epoch 18/50\n",
            "100/100 [==============================] - 4s 43ms/step - loss: 0.8756 - accuracy: 0.7328 - val_loss: 2.8706 - val_accuracy: 0.4200\n",
            "Epoch 19/50\n",
            "100/100 [==============================] - 3s 32ms/step - loss: 0.8198 - accuracy: 0.7494 - val_loss: 3.0508 - val_accuracy: 0.4238\n",
            "Epoch 20/50\n",
            "100/100 [==============================] - 3s 31ms/step - loss: 0.7685 - accuracy: 0.7597 - val_loss: 3.1291 - val_accuracy: 0.4212\n",
            "Epoch 21/50\n",
            "100/100 [==============================] - 4s 37ms/step - loss: 0.7280 - accuracy: 0.7759 - val_loss: 3.3816 - val_accuracy: 0.4238\n",
            "Epoch 22/50\n",
            "100/100 [==============================] - 5s 51ms/step - loss: 0.7028 - accuracy: 0.7825 - val_loss: 3.0082 - val_accuracy: 0.4225\n",
            "Epoch 23/50\n",
            "100/100 [==============================] - 6s 64ms/step - loss: 0.6781 - accuracy: 0.7891 - val_loss: 3.4413 - val_accuracy: 0.4250\n",
            "Epoch 24/50\n",
            "100/100 [==============================] - 4s 38ms/step - loss: 0.6390 - accuracy: 0.7966 - val_loss: 3.3132 - val_accuracy: 0.4487\n",
            "Epoch 25/50\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.5955 - accuracy: 0.8100 - val_loss: 3.5261 - val_accuracy: 0.4263\n",
            "Epoch 26/50\n",
            "100/100 [==============================] - 4s 38ms/step - loss: 0.5727 - accuracy: 0.8181 - val_loss: 3.4624 - val_accuracy: 0.4387\n",
            "Epoch 27/50\n",
            "100/100 [==============================] - 3s 31ms/step - loss: 0.5463 - accuracy: 0.8244 - val_loss: 3.8211 - val_accuracy: 0.4250\n",
            "Epoch 28/50\n",
            "100/100 [==============================] - 3s 35ms/step - loss: 0.5194 - accuracy: 0.8313 - val_loss: 3.5094 - val_accuracy: 0.4387\n",
            "Epoch 29/50\n",
            "100/100 [==============================] - 5s 47ms/step - loss: 0.5314 - accuracy: 0.8328 - val_loss: 3.6928 - val_accuracy: 0.4500\n",
            "Epoch 30/50\n",
            "100/100 [==============================] - 3s 30ms/step - loss: 0.5064 - accuracy: 0.8359 - val_loss: 4.0529 - val_accuracy: 0.4437\n",
            "Epoch 31/50\n",
            "100/100 [==============================] - 3s 30ms/step - loss: 0.5008 - accuracy: 0.8375 - val_loss: 3.7742 - val_accuracy: 0.4538\n",
            "Epoch 32/50\n",
            "100/100 [==============================] - 3s 31ms/step - loss: 0.5152 - accuracy: 0.8359 - val_loss: 4.3397 - val_accuracy: 0.4275\n",
            "Epoch 33/50\n",
            "100/100 [==============================] - 7s 71ms/step - loss: 0.4803 - accuracy: 0.8400 - val_loss: 3.8243 - val_accuracy: 0.4525\n",
            "Epoch 34/50\n",
            "100/100 [==============================] - 6s 58ms/step - loss: 0.4500 - accuracy: 0.8509 - val_loss: 3.8233 - val_accuracy: 0.4462\n",
            "Epoch 35/50\n",
            "100/100 [==============================] - 8s 80ms/step - loss: 0.4253 - accuracy: 0.8625 - val_loss: 4.0247 - val_accuracy: 0.4550\n",
            "Epoch 36/50\n",
            "100/100 [==============================] - 5s 49ms/step - loss: 0.4084 - accuracy: 0.8666 - val_loss: 3.9428 - val_accuracy: 0.4737\n",
            "Epoch 37/50\n",
            "100/100 [==============================] - 3s 30ms/step - loss: 0.4296 - accuracy: 0.8553 - val_loss: 3.9568 - val_accuracy: 0.4575\n",
            "Epoch 38/50\n",
            "100/100 [==============================] - 5s 46ms/step - loss: 0.4089 - accuracy: 0.8678 - val_loss: 3.9811 - val_accuracy: 0.4762\n",
            "Epoch 39/50\n",
            "100/100 [==============================] - 4s 37ms/step - loss: 0.3969 - accuracy: 0.8725 - val_loss: 4.0985 - val_accuracy: 0.4588\n",
            "Epoch 40/50\n",
            "100/100 [==============================] - 3s 33ms/step - loss: 0.3961 - accuracy: 0.8737 - val_loss: 4.5543 - val_accuracy: 0.4663\n",
            "Epoch 41/50\n",
            "100/100 [==============================] - 3s 31ms/step - loss: 0.4039 - accuracy: 0.8694 - val_loss: 4.3271 - val_accuracy: 0.4613\n",
            "Epoch 42/50\n",
            "100/100 [==============================] - 5s 45ms/step - loss: 0.3545 - accuracy: 0.8838 - val_loss: 4.2992 - val_accuracy: 0.4462\n",
            "Epoch 43/50\n",
            "100/100 [==============================] - 4s 37ms/step - loss: 0.3529 - accuracy: 0.8841 - val_loss: 4.4842 - val_accuracy: 0.4812\n",
            "Epoch 44/50\n",
            "100/100 [==============================] - 3s 30ms/step - loss: 0.3611 - accuracy: 0.8822 - val_loss: 4.7322 - val_accuracy: 0.4663\n",
            "Epoch 45/50\n",
            "100/100 [==============================] - 3s 30ms/step - loss: 0.3658 - accuracy: 0.8800 - val_loss: 4.3796 - val_accuracy: 0.4675\n",
            "Epoch 46/50\n",
            "100/100 [==============================] - 4s 41ms/step - loss: 0.3615 - accuracy: 0.8825 - val_loss: 4.4404 - val_accuracy: 0.4825\n",
            "Epoch 47/50\n",
            "100/100 [==============================] - 4s 41ms/step - loss: 0.3422 - accuracy: 0.8916 - val_loss: 4.6790 - val_accuracy: 0.4675\n",
            "Epoch 48/50\n",
            "100/100 [==============================] - 3s 31ms/step - loss: 0.3471 - accuracy: 0.8875 - val_loss: 4.6874 - val_accuracy: 0.4762\n",
            "Epoch 49/50\n",
            "100/100 [==============================] - 3s 32ms/step - loss: 0.3227 - accuracy: 0.8909 - val_loss: 5.4074 - val_accuracy: 0.4475\n",
            "Epoch 50/50\n",
            "100/100 [==============================] - 4s 37ms/step - loss: 0.3254 - accuracy: 0.8922 - val_loss: 4.8935 - val_accuracy: 0.4787\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 5.0479 - accuracy: 0.4600\n",
            "Accuracy: 0.46000000834465027\n"
          ]
        }
      ]
    }
  ]
}